{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing k-means clustering for Wikipedia Article Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is based on assignment in the Machine Learning specialization by University of Washington on Coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as ny \n",
    "import pandas as pd\n",
    "import math \n",
    "import operator\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people_wiki = pd.read_csv(\"people_wiki.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_km=people_wiki.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text=data_km['text']  ## to get the cell value from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count= []\n",
    "for i in range(0,len(text)):\n",
    "    text_words=text[i].split()\n",
    "    word_list = {}\n",
    "    for words in text_words:\n",
    "        if words in word_list:\n",
    "            word_list[words]=word_list[words]+1\n",
    "        else:\n",
    "            word_list[words]=1\n",
    "    word_count.append(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_km['word_count']=word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_idf = {} \n",
    "for names in data_km['name']:\n",
    "    curr=data_km[data_km['name']==names]\n",
    "    for words in curr.iloc[0]['word_count'].keys():\n",
    "        for i in range(0,len(data_km)):\n",
    "            if words in data_km.iloc[i]['word_count'].keys():\n",
    "                if words not in words_idf.keys():\n",
    "                    words_idf[words]=1\n",
    "                else:\n",
    "                    words_idf[words]=words_idf[words]+1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_list=[]\n",
    "for names in data_km['name']:\n",
    "    tf_idf={}\n",
    "    curr=data_km[data_km['name']==names]\n",
    "    for words in curr.iloc[0]['word_count'].keys():\n",
    "        tf_idf[words]=math.log((curr.iloc[0]['word_count'][words])/(1+words_idf[words]))\n",
    "    tf_idf_list.append(tf_idf)\n",
    "data_km['tf_idf']=tf_idf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of all the word that appear in the choosen articles\n",
    "text1=data_km['text']  ## to get the cell value from a dataframe\n",
    "word_list1= []\n",
    "for i in range(0,len(text1)):\n",
    "    text_words=text1[i].split()\n",
    "    for words in text_words:\n",
    "        if words not in word_list1:\n",
    "            word_list1.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stting the intial centroids\n",
    "centroid=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_size=math.inf\n",
    "for names in data_km['name']:\n",
    "    curr=data_km[data_km['name']==names]\n",
    "    for words in curr.iloc[0]['tf_idf'].keys():\n",
    "        if curr.iloc[0]['tf_idf'][words] < min_size:\n",
    "            min_size=curr.iloc[0]['tf_idf'][words]\n",
    "max_size=-math.inf\n",
    "for names in data_km['name']:\n",
    "    curr=data_km[data_km['name']==names]\n",
    "    for words in curr.iloc[0]['tf_idf'].keys():\n",
    "        if curr.iloc[0]['tf_idf'][words] > max_size:\n",
    "            max_size=curr.iloc[0]['tf_idf'][words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,6):\n",
    "    temp_list = {}\n",
    "    for words in word_list1:\n",
    "        temp_list[words]=random.uniform(min_size,max_size)\n",
    "    centroid.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_km['labels']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in range(0,3):    \n",
    "    for names in data_km['name']:\n",
    "            curr=data_km[data_km['name']==names]\n",
    "            zi=-1\n",
    "            dist_old=math.inf\n",
    "            for i in range(0,6):       \n",
    "                dist=0\n",
    "                for words in curr.iloc[0]['tf_idf'].keys():\n",
    "                    if words in centroid[i].keys():\n",
    "                        dist=dist+(centroid[i][words]-curr.iloc[0]['tf_idf'][words])*(centroid[i][words]-curr.iloc[0]['tf_idf'][words])\n",
    "                if dist < dist_old:\n",
    "                    dist_old=dist\n",
    "                    zi=i\n",
    "            data_km['labels'][data_km['name']==names]=zi \n",
    "\n",
    "    print('Count 0:', len(data_km[data_km['labels']==0]),'Count 1:', len(data_km[data_km['labels']==1]),'Count 2:', len(data_km[data_km['labels']==2]),'Count 3:', len(data_km[data_km['labels']==3]),'Count 4:', len(data_km[data_km['labels']==4]), 'Count 5:', len(data_km[data_km['labels']==5]))\n",
    "\n",
    "    for i in range(0,6):\n",
    "        for words in centroid[i].keys():\n",
    "            sum=0\n",
    "            for names in data_km['name'][data_km['labels']==i]:\n",
    "                curr=data_km[data_km['name']==names] \n",
    "                if words in curr.iloc[0]['tf_idf'].keys():\n",
    "                    sum=sum+curr.iloc[0]['tf_idf'][words]\n",
    "            centroid[i][words]=sum/len(data_km['name'][data_km['labels']==i]) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
